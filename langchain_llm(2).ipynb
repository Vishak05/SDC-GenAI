import gradio as gr
from langchain.chat_models import ChatOpenAI
from langchain_community.chat_models import ChatGoogleGenerativeAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# Set up API keys for the models
OPENAI_API_KEY = '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'

GOOGLE_API_KEY = 'AIzaSyAP8G6B_LKJKSFVvVI46LNhy_jQ_3IRuyM'

# Initialize OpenAI GPT-4 model
openai_llm = ChatOpenAI(model="gpt-4", openai_api_key=OPENAI_API_KEY)

# Initialize Google Gemini model (assumed method, check Langchain community for support)
gemini_llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=GOOGLE_API_KEY)

# Conversation buffers for each model
openai_memory = ConversationBufferMemory()
gemini_memory = ConversationBufferMemory()

# Conversation chains for each model
openai_chain = ConversationChain(llm=openai_llm, memory=openai_memory)
gemini_chain = ConversationChain(llm=gemini_llm, memory=gemini_memory)

# Function to choose between models based on user input (or you could set it as per your requirements)
def chatbot_fn(message, model_choice="OpenAI"):
    if model_choice == "OpenAI":
        return openai_chain.run(message)
    else:
        return gemini_chain.run(message)

# Gradio interface to interact with both models
def create_ui():
    with gr.Blocks() as demo:
        gr.Markdown("### Choose LLM Model and Chat")
        
        model_choice = gr.Radio(["OpenAI", "Google Gemini"], label="Select Model")
        input_message = gr.Textbox(label="Enter your message", lines=5)
        output_message = gr.Textbox(label="Response", lines=5)

        input_message.submit(chatbot_fn, inputs=[input_message, model_choice], outputs=output_message)

    return demo

# Launch the Gradio UI
ui = create_ui()
ui.launch(share=True)
